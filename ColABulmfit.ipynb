{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "ColABulmfit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ttHa5uMoKe-x",
        "VQD7U3whKe-2",
        "Lmvs4caqKe-4",
        "5F_rqnpgKe-9",
        "_A97y7-lKe_F",
        "4202jiWXKe_H",
        "d7aa_qQjKe_K",
        "95vX8e4jKe_Y",
        "YD99v4oaKe_d"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sleclair0/tweet-stance-prediction/blob/master/ColABulmfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5I9hmoGKe9m",
        "colab_type": "text"
      },
      "source": [
        "# Stance Classification of Tweets using Transfer Learning\n",
        "This notebook shows how *transfer learning*, an extension of deep learning, can be used for predicting Tweet stance toward a particular topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2i7JYv_Ke9o",
        "colab_type": "text"
      },
      "source": [
        "# 1. Motivation\n",
        "\n",
        "The traditional approach to applying deep learning methods in NLP have involved feeding a model large amounts of labeled training data, and fitting the model's parameters to this data. In practice, natural language data is highly variable and can come in a variety of forms (tweets, blog posts, reviews etc.), and hence, a model trained for a particular language task does not generalize well to new data from another distribution. In addition, many natural language applications do not come with an abundance of labeled examples, and human annotation can get very expensive as the datasets get larger.\n",
        "\n",
        "This offers good motivation to explore the notion of [transfer learning](http://ruder.io/transfer-learning/index.html#whatistransferlearning) - a machine-learning technique that has the ability to transfer knowledge to novel scenarios not encountered during training. While transfer learning has been ubiquitous throughout computer vision applications since the advent of huge datasets such as ImageNet, it is only since 2017-18 that significant progress has been made for transfer learning in NLP applications. There have been a string of interesting papers in 2018 that discuss the power of language models in natural language understanding and how they can be used to provide pre-trained representations of a language's syntax, which can be far more useful when training a neural network for previously unseen tasks.\n",
        "\n",
        "Twitter data is a very interesting use case for transfer learning, mainly because the typical language syntax seen in Tweets is quite different from that which is used to train language models. For these reasons, the **2016 SemEval Stance Detection task** is chosen for studying the effectiveness of our transfer learning approach. The dataset, experiments and the evaluation criteria used are explained in below sections. \n",
        "\n",
        "The aim of this notebook is to highlight the development of a model that can help answer the following questions:\n",
        "- How does our approach generalize to Twitter-specific language syntax?\n",
        "- Are we able to achieve reasonable results (comparable to the winning team of SemEval 2016 Task 6) with *limited amounts of training data* and *limited computing resources*?\n",
        "- How much fine-tuning effort is required to achieve reasonable results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs3_q1umKe9p",
        "colab_type": "text"
      },
      "source": [
        "# 2. Approach\n",
        "This section describes the 3-Layer LSTM architecture used for transfer learning, **ULMFit**. This is the architecture described in the well known paper by [Howard and Ruder, 2018](https://arxiv.org/pdf/1801.06146.pdf), that has been proven to generalize\n",
        "well to unseen distributions. The goal is to see how *ULMFit* can perform in this particular Tweet stance classification task.\n",
        "\n",
        "\n",
        "## 2.1 ULMFit Model Architecture\n",
        "![title](https://github.com/sleclair0/tweet-stance-prediction/blob/master/assets/ulmfit_arch.png?raw=1)\n",
        "\n",
        "Source: [Howard and Ruder, 2018](https://arxiv.org/pdf/1801.06146.pdf)\n",
        "\n",
        "To classify out-of-domain data (such as our Twitter data), the following techniques are applied from the ULMFit paper:\n",
        "\n",
        "### Discriminative Fine-tuning\n",
        "From the *ULMFit* paper, each layer of the model captures *different types of information*. Hence, it makes sense to fine-tune each layer differently, and this is done in *ULMFit* after extensive empirical testing and implementation updates. Rather than change each layer manually, we can use the implemented ratios, but adapt the multiplier that influences the learning rate for each layer. \n",
        "\n",
        "*ULMFit* also uses discriminative fine-tuning with regard to the SGD update as follows:\n",
        "\n",
        "$\\theta_{t}^{l} = \\theta_{t-1}^{l} - \\eta^{l} \\cdot \\nabla_{\\theta^l} J{\\theta} $\n",
        "\n",
        "### 1-cycle learning rate policy\n",
        "In the fine-tuning stage, a *1-cycle learning rate* policy is applied, which comes from this [report by Leslie Smith](https://arxiv.org/abs/1803.09820). It is a modification of the cyclical learning rate policy, which has been around for a long time, but the 1-cycle policy allows a large initial learning rate ($LR_{max}=10^{-3}$, for example), but decreases it by several orders of magnitude just at the last epoch. This seems to provide greater final accuracy. In the ULMFit implementation,  this 1-cycle policy has been tweaked and is referred to as *slanted triangular learning rate*.\n",
        "\n",
        "### Gradual unfreezing\n",
        "Rather than training all the layers at once during classification, the layers are \"frozen\" and the last layer is fine-tuned first, followed by the next layer before it, and so on. This avoids the phenomenon known as *catastrophic forgetting* (by fine-tuning all layers too aggressively). \n",
        "\n",
        "### Concatenated pooling\n",
        "Because an input text can consist of hundreds or thousands of words, information might get lost if we only consider the last hidden state.\n",
        "\n",
        "Hence, the hidden state at the last time step, $h_T$ is concatenated with *both* the max-pooled and mean-pooled representation of the hidden states over as many time steps as can fit in GPU memory.\n",
        "\n",
        "$h_C = [h_T, maxpool(H), meanpool(H)]$\n",
        "\n",
        "Where $H$ is the vector of all hidden states.\n",
        "\n",
        "### 3-stage fine-tuning methodology\n",
        "The classification task is done in a 3-stage process:\n",
        "1. General-domain LM pretraining: ULMFit has a pretrained model generated using an AWD-LSTM (as per [Merity et al., 2017]((https://arxiv.org/pdf/1708.02182.pdf))) to develop a language model called ```Wikitext-103``` and was trained of 28,595 preprocessed Wikipedia articles, totalling to 103 million words. \n",
        "2. Target task *LM* fine-tuning: Since the data for the target will likely come from a different distribution, ULMFit allows us to use the pre-trained language model anf fine-tune it (using the above techniques) to adapt to the idiosyncrasies of the target data.\n",
        "3. Target task *classifier* fine-tuning: Once we save the updated weights from the language model fine-tuning step, we can fine-tune the classifier with gradual unfreezing and the other techniques described above to perform  task-specific class prediction. \n",
        "\n",
        "Note that for this SemEval Tweet stance classification task, we only perform steps 2 and 3, and utilize the pretrained language model from the ```fastai``` database. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGkPegREKe9q",
        "colab_type": "text"
      },
      "source": [
        "# 3. Data\n",
        "\n",
        "The training and test data consist of Tweets pertaining to five distinct topics, shown in [SemEval 2016: Task 6](http://alt.qcri.org/semeval2016/task6/). We only look at Task A: \"Supervised Framework\" in this notebook. The train and test data (including the gold) are in the the ```data/``` directory provided along with this repository.\n",
        "\n",
        "The five topics for which Tweets are provided as part of the training data are given below. \n",
        "\n",
        "| Topic        \n",
        "|:------------ | \n",
        "| Atheism     |\n",
        "| Climate Change is a Real Concern  | \n",
        "| Feminist Movement | \n",
        "| Hillary Clinton  |\n",
        "| Legalization of Abortion  | \n",
        "\n",
        "A more detailed breakdown of the tweets for this shared task is provided in [this link](http://www.saifmohammad.com/WebPages/StanceDataset.htm). \n",
        "\n",
        "## Size of dataset\n",
        "The total number of Tweets (in the training set) available for this task is roughly 2700, which amounts to roughly 500-600 Tweets per topic. Thus, this can be considered a small dataset. \n",
        "\n",
        "![title](https://github.com/sleclair0/tweet-stance-prediction/blob/master/assets/stance_balance.png?raw=1)\n",
        "\n",
        "Upon inspecting the training data, it is clear that there is quite a large variance in terms of the number of Tweets in favor vs. those against a topic. There is quite a large variance *within* classes as well as *between* classes in the overall data. \n",
        "\n",
        "## 3.1 Pretrained language models\n",
        "\n",
        "*ULMFit* uses its own pretrained language model ```wikitext-103``` that can be conveniently downloaded  from the ```fastai``` database hosted on AWS. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg3ZQj_tKe9q",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Preprocess SemEval training data\n",
        "This section discusses the steps used to prepare the data for transfer learning utilizing the ```fastai``` framework. In particular, the ```fastai.text``` library is designed to work very well with tabular data, hence, Pandas DataFrames are used to organize and filter the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TR80yYCKtPY",
        "colab_type": "code",
        "outputId": "a3decf53-9288-412e-91be-173b425545ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "!ls\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LICENSE\t\t\t  README.md\t    sample_data        ulmfit.ipynb\n",
            "pytorch-requirements.txt  requirements.txt  transformer.ipynb  vocab.pkl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.24.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (2.1.6)\n",
            "Collecting nbimporter (from -r requirements.txt (line 8))\n",
            "  Downloading https://files.pythonhosted.org/packages/39/a1/9771984f60c4ea6d0c36215b44a5c1537cfda845c08e7ee5cf748c43d7cf/nbimporter-0.3.1.tar.gz\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r requirements.txt (line 6)) (0.21.3)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (2.21.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (0.0.7)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 7)) (7.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 3)) (41.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 3)) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 6)) (0.13.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (2.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy->-r requirements.txt (line 7)) (4.28.1)\n",
            "Building wheels for collected packages: nbimporter\n",
            "  Building wheel for nbimporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nbimporter: filename=nbimporter-0.3.1-cp36-none-any.whl size=3792 sha256=f508dfc22e8ea6133d751347f90b70be27a5c8a80069a701227e5473411fcddf\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/7d/40/f48fe3f0995a282d582ca73a6807b9bb23c8cc3f1525e62429\n",
            "Successfully built nbimporter\n",
            "Installing collected packages: nbimporter\n",
            "Successfully installed nbimporter-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKpRo7yCMIBP",
        "colab_type": "code",
        "outputId": "f6b8eaff-d149-4da1-a09b-7d4a7e2dcda9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!pip3 install -r pytorch-requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r pytorch-requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from -r pytorch-requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r pytorch-requirements.txt (line 3)) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->-r pytorch-requirements.txt (line 1)) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->-r pytorch-requirements.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->-r pytorch-requirements.txt (line 2)) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->-r pytorch-requirements.txt (line 2)) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlCLVGJ1MYlz",
        "colab_type": "code",
        "outputId": "d7245a8b-8ecc-43d4-848b-ec360117bc82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python -V"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "05e6f60c-ea8e-4d4c-edf2-0b5b7fe8c298",
        "id": "MLTdSoAaUapQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Check that the latest 1.0.0 build of PyTorch has been installed \n",
        "# alongside fastai\n",
        "import torch\n",
        "print(\"Cuda available\" if torch.cuda.is_available() is True else \"CPU\")\n",
        "print(\"PyTorch version: \", torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available\n",
            "PyTorch version:  1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHjHSgJNNPjO",
        "colab_type": "code",
        "outputId": "67df8e60-5071-4eb6-a2d6-410cacb771e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "!pip3 install fastai"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.55)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.16.4)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (19.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.0.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.6.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.24.2)\n",
            "Requirement already satisfied: typing; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (3.7.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.21)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (7.0.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (1.12.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (19.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.0.18->fastai) (4.28.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep_PKmJgKe9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3vDhHJbNYmn",
        "colab_type": "code",
        "outputId": "81aa1dbb-ed14-41d3-e7d8-6d90c507a4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!python3 -m spacy download en\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiZyoaJLPfnv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bf79ce07-49f5-4b4e-da23-d950f9a459ee"
      },
      "source": [
        "!mkdir data\n",
        "!mkdir ./data/eval\n",
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t\t  README.md\t    transformer.ipynb\n",
            "LICENSE\t\t\t  requirements.txt  ulmfit.ipynb\n",
            "pytorch-requirements.txt  sample_data\t    vocab.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzs7Xtr8QLn2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f7faa67c-8433-4093-ee47-adaad76bfcd0"
      },
      "source": [
        "print(dir(URLs))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ADULT_SAMPLE', 'AG_NEWS', 'AMAZON_REVIEWS', 'AMAZON_REVIEWS_POLARITY', 'BIWI_HEAD_POSE', 'BIWI_SAMPLE', 'CALTECH_101', 'CAMVID', 'CAMVID_TINY', 'CARS', 'CIFAR', 'CIFAR_100', 'COCO_SAMPLE', 'COCO_TINY', 'CUB_200_2011', 'DBPEDIA', 'DOGS', 'FLOWERS', 'FOOD', 'HUMAN_NUMBERS', 'IMAGENETTE', 'IMAGENETTE_160', 'IMAGENETTE_320', 'IMAGEWOOF', 'IMAGEWOOF_160', 'IMAGEWOOF_320', 'IMDB', 'IMDB_SAMPLE', 'LOCAL_PATH', 'LSUN_BEDROOMS', 'ML_SAMPLE', 'MNIST', 'MNIST_SAMPLE', 'MNIST_TINY', 'MNIST_VAR_SIZE_TINY', 'MT_ENG_FRA', 'OPENAI_TRANSFORMER', 'PASCAL_2007', 'PASCAL_2012', 'PETS', 'PLANET_SAMPLE', 'PLANET_TINY', 'S3', 'S3_COCO', 'S3_IMAGE', 'S3_IMAGELOC', 'S3_MODEL', 'S3_NLP', 'SOGOU_NEWS', 'WIKITEXT', 'WIKITEXT_TINY', 'WT103_BWD', 'WT103_FWD', 'YAHOO_ANSWERS', 'YELP_REVIEWS', 'YELP_REVIEWS_POLARITY', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOeocwHHKe9t",
        "colab_type": "text"
      },
      "source": [
        "### Define the path to train and test data for SemEval 2016 Task 6\n",
        "The full data and evaluation files for this task is available on the [source page](http://alt.qcri.org/semeval2016/task6/index.php?id=data-and-tools). However, these have been included along with this repository for future reference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH7iSIrjKe9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = Path('./data')\n",
        "trainfile = 'semeval2016-task6-trainingdata.txt'\n",
        "testfile = 'SemEval2016-Task6-subtaskA-testdata.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrxvkTO7Ke9w",
        "colab_type": "text"
      },
      "source": [
        "Some of the characters in the training set are not *utf-8* encoded, hence we make sure that we only treat the characters that appear in the English language for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNjSz3dIKe9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_ascii(text):\n",
        "    # function to remove non-ASCII chars from data\n",
        "    return ''.join(i for i in text if ord(i) < 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ge7aTr2Ke9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_orig = pd.read_csv(path/trainfile, delimiter='\\t', header=0, encoding = \"latin-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXqxbeeUKe91",
        "colab_type": "code",
        "outputId": "05e6f60c-ea8e-4d4c-edf2-0b5b7fe8c298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Check that the latest 1.0.0 build of PyTorch has been installed \n",
        "# alongside fastai\n",
        "import torch\n",
        "print(\"Cuda available\" if torch.cuda.is_available() is True else \"CPU\")\n",
        "print(\"PyTorch version: \", torch.__version__)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available\n",
            "PyTorch version:  1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W06h39mDKe94",
        "colab_type": "code",
        "outputId": "fc5c758a-8997-415b-e009-f76e18fd9b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "# Plot value counts for rach stance in the dataset\n",
        "train_orig['Stance'].value_counts().plot.bar(rot=30);"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAERCAYAAABy/XBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6ikDQJJ66v8Bih9w2Bve2JwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abKkbS1kKe97",
        "colab_type": "code",
        "outputId": "9466010e-cfc9-40d8-8fbf-88d788f4b0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"SemEval 2016 Task 6 (subtask A) Tweet topics are:\\n\")\n",
        "print(\"------------------------------------------\")\n",
        "for item in train_orig['Target'].unique():\n",
        "    print(item)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SemEval 2016 Task 6 (subtask A) Tweet topics are:\n",
            "\n",
            "------------------------------------------\n",
            "Atheism\n",
            "Climate Change is a Real Concern\n",
            "Feminist Movement\n",
            "Hillary Clinton\n",
            "Legalization of Abortion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uthwR1jKe99",
        "colab_type": "text"
      },
      "source": [
        "### Apply the cleaning function to the 'Tweet' column of the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY-i0QmzKe9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_orig['Tweet'] = train_orig['Tweet'].apply(clean_ascii)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYAQO3SNKe-A",
        "colab_type": "code",
        "outputId": "1c7dfcd0-256d-4c49-ee1d-a1f970e3a840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_orig.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Target</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>101</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>dear lord thank u for all of ur blessings forg...</td>\n",
              "      <td>AGAINST</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>102</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>Blessed are the peacemakers, for they shall be...</td>\n",
              "      <td>AGAINST</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>103</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>I am not conformed to this world. I am transfo...</td>\n",
              "      <td>AGAINST</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>104</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>Salah should be prayed with #focus and #unders...</td>\n",
              "      <td>AGAINST</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>105</td>\n",
              "      <td>Atheism</td>\n",
              "      <td>And stay in your houses and do not display you...</td>\n",
              "      <td>AGAINST</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ID   Target                                              Tweet   Stance\n",
              "0  101  Atheism  dear lord thank u for all of ur blessings forg...  AGAINST\n",
              "1  102  Atheism  Blessed are the peacemakers, for they shall be...  AGAINST\n",
              "2  103  Atheism  I am not conformed to this world. I am transfo...  AGAINST\n",
              "3  104  Atheism  Salah should be prayed with #focus and #unders...  AGAINST\n",
              "4  105  Atheism  And stay in your houses and do not display you...  AGAINST"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVvGaglbKe-C",
        "colab_type": "text"
      },
      "source": [
        "### Store the relevant information from training data \n",
        "ULMFit requires just the stance and the text data (i.e. tweets) for the language-model finetuning and classification steps, hence these are stored in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLcdljCfKe-D",
        "colab_type": "code",
        "outputId": "87d77faf-4fa0-4dc3-917b-5f345a77bbfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train = pd.concat([train_orig['Stance'], train_orig['Tweet']], axis=1)\n",
        "train.tail()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Stance</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2809</th>\n",
              "      <td>AGAINST</td>\n",
              "      <td>There's a law protecting unborn eagles, but no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2810</th>\n",
              "      <td>AGAINST</td>\n",
              "      <td>I am 1 in 3... I have had an abortion #Abortio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2811</th>\n",
              "      <td>AGAINST</td>\n",
              "      <td>How dare you say my sexual preference is a cho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2812</th>\n",
              "      <td>AGAINST</td>\n",
              "      <td>Equal rights for those 'born that way', no rig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2813</th>\n",
              "      <td>AGAINST</td>\n",
              "      <td>#POTUS seals his legacy w/ 1/2 doz wins. The #...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Stance                                              Tweet\n",
              "2809  AGAINST  There's a law protecting unborn eagles, but no...\n",
              "2810  AGAINST  I am 1 in 3... I have had an abortion #Abortio...\n",
              "2811  AGAINST  How dare you say my sexual preference is a cho...\n",
              "2812  AGAINST  Equal rights for those 'born that way', no rig...\n",
              "2813  AGAINST  #POTUS seals his legacy w/ 1/2 doz wins. The #..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duqhaDXYKe-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write train to csv\n",
        "train.to_csv(path/'train.csv', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q4EvMMxKe-H",
        "colab_type": "text"
      },
      "source": [
        "# 4. Experimental Setup\n",
        "\n",
        "Stance detection uses a *semi-supervised approach* where we reuse weights from a pretrained language model (in this case ```wikitext-103```) and perform multi-class classification on the training data over the three classes (*FAVOR*, *AGAINST* and *NONE*).\n",
        "\n",
        "### Evaluation\n",
        "The metric used to score the stance classification is **F-score**. The event organizers provide an [evaluation script](http://alt.qcri.org/semeval2016/task6/index.php?id=data-and-tools) that calculates the macro-average of F-score (FAVOR) and F-score (AGAINST) for task A. This compares our model's predicted stance for each Tweet against the gold reference.\n",
        "\n",
        "The *perl* script provided by the organizers is used to generate an F- score. The evaluation script is in ```data/eval/``` and has the following usage:\n",
        "    \n",
        "    cd data/eval\n",
        "    perl eval.pl -u\n",
        "\n",
        "    ---------------------------\n",
        "    Usage:\n",
        "    perl eval.pl goldFile guessFile\n",
        "\n",
        "    goldFile:  file containing gold standards;\n",
        "    guessFile: file containing your prediction.\n",
        "\n",
        "### Reference result\n",
        "To have a benchmark to compare our results against, it makes sense to look at the winning paper for this shared task, from team *MITRE*, who [published their methodology and approach](https://arxiv.org/pdf/1606.03784.pdf). From this paper, it can be seen that *MITRE* used an LSTM-based deep learning approach and performed per-topic classification of Tweets to achieve an average macro F-score of **0.67**. \n",
        "\n",
        "### Stance Prediction\n",
        "The predicted output stances on the test dataset is written out according to the format expected by the evaluation *perl* script, and the F-scores are published as per this evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nwxA-tkKe-I",
        "colab_type": "text"
      },
      "source": [
        "# 5. Transfer Learning Stages\n",
        "\n",
        "This notebook utilizes the refactored and updated version (```v1```) of *ULMFit* as implemented in the [fastai](https://github.com/fastai/fastai/tree/master/fastai)  GitHub repository. This requires an install of **Pytorch 1.0.0** ([released on December 7, 2018](https://developers.facebook.com/blog/post/2018/05/02/announcing-pytorch-1.0-for-research-production/)) to run since its development intended to move alongside PyTorch's own development cycle in the future. It is recommended to run this entire notebook in a virtual environment to avoid clashes with pre-existing installs of PyTorch.\n",
        "\n",
        "The refactored code for *ULMFit* is much more high-level and abstracted than the original version that was used in the paper, so it doesn't make sense to rewrite the data manipulation code for text handling and tokenization from the ground up. We can take advantage of the powerful transfer learning functionality that is baked into ULMFit (obtained after months of fine-tuning and refactoring the source code by Fast.ai) instead! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1guRxd1yKe-I",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Language model finetuning: (SemEval Tweets only)\n",
        "The pre-trained language model used is from [this work by Merity et al.](https://arxiv.org/pdf/1708.02182.pdf). Although we are working with Twitter whose content's structure that can be quite dissimilar to Wikipedia's sentence formulation, the hope is that the language understanding developed by the model, in combination with the fine-tuning techniques described in [Howard and Ruder's paper](https://arxiv.org/pdf/1801.06146.pdf), will allow us to at least transfer some knowledge for the purposes of classification of Tweet stance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3UnWXG2Ke-J",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess data for language model fine-tuning\n",
        "The ```TextLMDataBunch```[ [source]](https://docs.fast.ai/text.data.html#TextLMDataBunch) class returns a \"Databunch\" type of object that is a custom object defined in fastai.text's API. Note that according to this class definition, a significant amount of data pre-processing is going on under the hood. \n",
        "\n",
        "- Initialize the language model with the pre-trained vocabulary's weights\n",
        "- Create a custom data loader that \"batchifies\" the target task's vocabulary, i.e. split it up into batches for processing on the GPU\n",
        "- Introduce randomness into the batches, but without breaking up the sentence sequences using techniques [borrowed from language modeling](https://arxiv.org/pdf/1708.02182.pdf).\n",
        "- Tokenize the target task's vocabulary and increase the efficiency of tokenization (for large tasks) using multi-threading\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLi1Cup1Ke-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Language model data\n",
        "data_lm = TextLMDataBunch.from_csv(path, 'train.csv', min_freq=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMgiArYPKe-M",
        "colab_type": "text"
      },
      "source": [
        "Note that in this case, we specify a minimum word frequency of 1 (i.e. words that appear just once in the data are given the tag ```<unk>``` during tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIqEBZ7xKe-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the language and classifier model data for re-use\n",
        "data_lm.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVUzBrJ-Ke-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Reload LM and clas data from file if doing so for large cases\n",
        "# data_lm = TextLMDataBunch.load(path)\n",
        "# data_clas = TextClasDataBunch.load(path, bs=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kox9e78Ke-R",
        "colab_type": "code",
        "outputId": "49d67d56-a947-49b3-daf5-f34a61858e06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "data_lm.show_batch()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>fruit . # semst xxbos xxmaj really interesting debate about whether men can also be feminists # bscconf15 # criminology # semst xxbos xxmaj take your # destiny out of the hands of people &amp; place it in the hands of xxmaj god . # semst xxbos # xxmaj republican party will go down in history books as party that stood in the way of # gayrights and initiative to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>'s sports , something fairly important is happening now with xxup usa women 's soccer # semst xxbos xxmaj regards xxmaj dion # peace to you &amp; all # including into the # future on a clearly # known to be # finiteinnatureplanet outta here gone # semst xxbos xxmaj idiot : how would you feel if your mom aborted you ? xxmaj me : nothing cause i would be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>proud to be # xxup uniteblue~ # waronwomen # worldpeace # tngov # xxup tn # socialjustice # alllivesmatter # unity # semst xxbos xxmaj towie catchup , totally agree with ferne # xxup towie # semst xxbos xxmaj that 's like being into cannibalism but against abortion.comparing being against eggs but loving chicken . # cannibalism # analogy # semst xxbos # magicmikexxl is the most feminist movie i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td># bellhooks # love # semst xxbos @andrea_provida xxmaj or another hypocrisy are those who campaign for animal rights but are \" pro choice \" . # semst xxbos xxmaj interesting in the video she kidnaps and tortures(tying up , confinement , etc ) the wife or xxup gf of the evil accountant # semst xxbos xxmaj woman : you see me as a sex object ! xxmaj me :</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>slain and resurrected can save xxmaj america . xxmaj behold the xxmaj lamb ! # semst xxbos @__hiskindacrazy @_queentacobell @thumperbunny88 xxmaj abortion is xxup not illegal and i would never judge a woman for making that choice . # semst xxbos xxmaj cruz-- xxmaj perry 2016 @tedcruz @governorperry xxmaj it 's the only win - win choice # semst xxbos @bdutt @lalitkmodi @mohdasim1 ohh .so i think why r u</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPoPznGaKe-T",
        "colab_type": "text"
      },
      "source": [
        "We only need to run the above tokenization step once, and save the data. All the data goes to a local folder named ```tmp/```. For a large added vocabulary, the above tokenization step can take a while, hence we save them and restart the notebook from the next cell (if we close our session).\n",
        "\n",
        "The tokenization technique used by ```fastai.text``` is quite advanced, and uses quite a few tricks to capture semantic meaning from the text. Unlike typical NLTK-type tokenization, here we are not converting the Tweets to lowercase and removing information from the base text - that would result in a tremendous loss of information that our model could use to gather an understanding of our new task's vocabulary.\n",
        "\n",
        "Instead, a number of added tokens are applied so that minimal information is lost. A full list of all the tokenization rules used are [here](https://docs.fast.ai/text.transform.html#Rules). \n",
        "\n",
        "As an example, the ```xxmaj``` token [[source]](https://github.com/fastai/fastai/blob/1c5c007e10a187f4a048a3ee8137016534259768/fastai/text/transform.py#L71) indicates that there is capitalization of the word, either in part or in full. \"The\" will be tokenized as \"xxmaj the\". For words that are full capitalized, such as \"I AM SHOUTING\"; it is tokenized as \"xxup i xxup am xxup shouting\". The tokenizer uses spaCy's underlying tokens, but adds tags in a very smart way that balances capturing semantic meaning while reducing the number of overall tokens, so it is both powerful and efficient. \n",
        "\n",
        "The tokenization process described above is highly efficient, in part thanks to the power of *SpaCy* and also due to the multi-threading wrappers implemented by Fast.ai on top of Spacy's tokenizer. As a result, a really large language model's vocabulary (of millions of words) can also be effectively tokenized in a reasonable time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWEogaASKe-U",
        "colab_type": "text"
      },
      "source": [
        "### Define learner object for language model finetuning\n",
        "We define a learner object that uses the tokenized language model data, that is organized into batches for the GPU, and feed it a pre-trained language model. \n",
        "\n",
        "We use the ```wt_103_v1``` model that was refined after re-training using the more efficient ```fastai v1``` library. The data (```lstm_wt103.pth```) is downloaded through an AWS-hosted link (accessed through the ```fastai.datasets``` interface), and stored locally. All the data in the language model file is numericalized and stored in a way that the learner can efficiently pick up tokens in float form. In addition to the language model, there is also an additional pickle object ```itos_wt103.pkl``` that is similar to the ```torchtext``` definition of \"integer-to-string\" - this contains the list of unique tokens in word form, so that we can map the tokens back to strings if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L10Y73pXad5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16GDruZ7Ke-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, pretrained=True, drop_mult=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGa8UJRdKe-Z",
        "colab_type": "text"
      },
      "source": [
        "### Print the structure of the language model RNN \n",
        "We can examine the architecture of the 3-layer LSTM used in ULMFit as shown below. For the language model fine-tuning, we define the embeddings based on the the target task vocabulary size. A single linear output layer is defined  that updates the weights once we pass the input through the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGLmn9nXKe-b",
        "colab_type": "code",
        "outputId": "ed6a055a-31fb-4903-818d-8286ced57cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "list(learn.model.children())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AWD_LSTM(\n",
              "   (encoder): Embedding(7984, 400, padding_idx=1)\n",
              "   (encoder_dp): EmbeddingDropout(\n",
              "     (emb): Embedding(7984, 400, padding_idx=1)\n",
              "   )\n",
              "   (rnns): ModuleList(\n",
              "     (0): WeightDropout(\n",
              "       (module): LSTM(400, 1152, batch_first=True)\n",
              "     )\n",
              "     (1): WeightDropout(\n",
              "       (module): LSTM(1152, 1152, batch_first=True)\n",
              "     )\n",
              "     (2): WeightDropout(\n",
              "       (module): LSTM(1152, 400, batch_first=True)\n",
              "     )\n",
              "   )\n",
              "   (input_dp): RNNDropout()\n",
              "   (hidden_dps): ModuleList(\n",
              "     (0): RNNDropout()\n",
              "     (1): RNNDropout()\n",
              "     (2): RNNDropout()\n",
              "   )\n",
              " ), LinearDecoder(\n",
              "   (decoder): Linear(in_features=400, out_features=7984, bias=True)\n",
              "   (output_dp): RNNDropout()\n",
              " )]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r--XDBOQKe-d",
        "colab_type": "text"
      },
      "source": [
        "### Find the optimum learning rate \n",
        "```fastai.train``` [source](https://github.com/fastai/fastai/blob/050080be574cb1260462bbd03e9600e43e7a54b1/fastai/train.py#L23) provides a convenient utility to search through a range of learning rates to find the optimum one for our dataset. \n",
        "\n",
        "The idea is that our optimization function needs to use a learning rate that is an order of magnitude below the point at which the loss starts to diverge. We can quickly find this as shown below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NN2qUp-Ke-d",
        "colab_type": "code",
        "outputId": "3c8c1ad5-34fb-472a-a463-d3083090bacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "learn.lr_find(start_lr=1e-8, end_lr=1e2)\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-po3gItKe-f",
        "colab_type": "text"
      },
      "source": [
        "### Run the ```language_model_learner``` class\n",
        "The pre-trained language model quickly downloads from the ```fastai```'s AWS link, so there is no need to download it separately. [```drop_mult```] is the multiplier applied to the dropout in the language learner. As per the ULMFit paper, we use a custom dropout for each language model layer, and the ```drop_mult```just scales a [numpy array of these dropouts](https://github.com/fastai/fastai/blob/1c5c007e10a187f4a048a3ee8137016534259768/fastai/text/learner.py#L15) which remain in the same relative ratio to one another. These numbers have been obtained through empirical tuning for a range of language tasks, and we took note of this during our extensive reading on this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2avqiJOKe-g",
        "colab_type": "code",
        "outputId": "8e0fc8ea-1187-43d8-c4d5-48719fba0fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, pretrained=True, \n",
        "                               drop_mult=0.5)\n",
        "learn.fit_one_cycle(cyc_len=1, max_lr=1e-3, moms=(0.8, 0.7))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.521700</td>\n",
              "      <td>5.625101</td>\n",
              "      <td>0.170815</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVqlRARkKe-j",
        "colab_type": "text"
      },
      "source": [
        "### Unfreeze the entire model \n",
        "To train the language model during fine-tuning, we unfreeze all the layers and then perform training for a reasonable time until we obtain a low enough validation loss. For a larger target vocabulary, these hyperparameters might need to be modified, and teh training time would be significantly longer, but for this SemEval task, we have a very small additional vocabulary to fine-tune the language model on, so it is really quick.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-vIrFY8Ke-j",
        "colab_type": "code",
        "outputId": "91e3311b-575b-4ccb-d6ab-708def5eb8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(cyc_len=20, max_lr=1e-3, moms=(0.8, 0.7))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.977990</td>\n",
              "      <td>4.777051</td>\n",
              "      <td>0.230301</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.599826</td>\n",
              "      <td>4.612112</td>\n",
              "      <td>0.229185</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.334851</td>\n",
              "      <td>4.565337</td>\n",
              "      <td>0.253404</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.135269</td>\n",
              "      <td>4.539562</td>\n",
              "      <td>0.268806</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.949691</td>\n",
              "      <td>4.487193</td>\n",
              "      <td>0.263895</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.742490</td>\n",
              "      <td>4.373820</td>\n",
              "      <td>0.301172</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>4.536380</td>\n",
              "      <td>4.287908</td>\n",
              "      <td>0.303683</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>4.305500</td>\n",
              "      <td>4.310244</td>\n",
              "      <td>0.305413</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>4.067547</td>\n",
              "      <td>4.357085</td>\n",
              "      <td>0.303069</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.831058</td>\n",
              "      <td>4.393252</td>\n",
              "      <td>0.306417</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.608122</td>\n",
              "      <td>4.498805</td>\n",
              "      <td>0.304520</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.385843</td>\n",
              "      <td>4.529005</td>\n",
              "      <td>0.302511</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.167734</td>\n",
              "      <td>4.579029</td>\n",
              "      <td>0.302288</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.976690</td>\n",
              "      <td>4.670191</td>\n",
              "      <td>0.301618</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.798164</td>\n",
              "      <td>4.693319</td>\n",
              "      <td>0.298828</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.646522</td>\n",
              "      <td>4.720749</td>\n",
              "      <td>0.300335</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.518104</td>\n",
              "      <td>4.740119</td>\n",
              "      <td>0.297600</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.410336</td>\n",
              "      <td>4.760403</td>\n",
              "      <td>0.298214</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.331965</td>\n",
              "      <td>4.755454</td>\n",
              "      <td>0.298214</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.262673</td>\n",
              "      <td>4.751477</td>\n",
              "      <td>0.298828</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-CYJmOfKe-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the fine-tuned encoder\n",
        "learn.save_encoder('ft_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6o1Hu9TKe-m",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 Train the Classifier: SemEval Tweets only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSjdu6QAKe-n",
        "colab_type": "text"
      },
      "source": [
        "For classification, the [winning paper for the SemEval 2016](https://www.aclweb.org/anthology/S/S16/S16-1074.pdf) task (team MITRE) describe in section $5$ of their paper that they achieved their highest average F1 score when they trained 5 *distinct* classifiers, i.e. a separate training task for each of the 5 topics in the training data. \n",
        "\n",
        "Hence, in this notebook we only perform classification for one *topic* at a time. Evaluation of the predicted classes is also done on a per-topic basis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I61Kf4CKe-q",
        "colab_type": "text"
      },
      "source": [
        "### Identify the 5 topics in our training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AAORFu7Ke-q",
        "colab_type": "code",
        "outputId": "782b8658-d453-4c1d-f121-fe992ade00fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for item in train_orig.Target.unique():\n",
        "    print(item)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Atheism\n",
            "Climate Change is a Real Concern\n",
            "Feminist Movement\n",
            "Hillary Clinton\n",
            "Legalization of Abortion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E8iLdj-Ke-s",
        "colab_type": "text"
      },
      "source": [
        "### Specify the topic we want to classify for\n",
        "We specify the topic name exactly as the training data does, and use it to subset the Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q5W78hGKe-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic = \"Feminist Movement\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-sAR8UGKe-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "3aea5538-c592-4907-fc59-f7d61683f01c"
      },
      "source": [
        "# Get only those tweets that pertain to a single topic in the training data\n",
        "train_topic = train.loc[train_orig['Target'] == topic]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e756a755a074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'fastai.train' has no attribute 'loc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvyOnjDnKe-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write train to csv\n",
        "train_topic.to_csv(path/'train_topic.csv', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttHa5uMoKe-x",
        "colab_type": "text"
      },
      "source": [
        "### Create classification dataloader\n",
        "For classification, the ```TextClasDataBunch```  [[source](https://docs.fast.ai/text.data.html#TextClasDataBunch)] dataloader used is slightly different. Just like the language model custom data loader described in the previous section, it does a lot under the hood. The classification data loader tokenizes the training data, but batchifies the data differently from the language model data loader - for classification, we perform sorting so as to group all large sentences together, and smaller sentences together - and pad them to have all batches of roughly the same length, followed by randomizing these batches so that we don't introduce additional bias during training.\n",
        "\n",
        "Just like before, we define the learner object used for classification. Note that we feed in the entire vocabulary of the training data to the classifier. We once more specify a minimum word frequency of 1 (i.e. words that appear just once in the data are given the tag ```<unk>```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FaaySn-Ke-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier model data\n",
        "data_clas = TextClasDataBunch.from_csv(path, 'train_topic.csv', vocab=data_lm.train_ds.vocab,\n",
        "                                       min_freq=1, bs=32)\n",
        "data_clas.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BePwIR24Ke-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = text_classifier_learner(data_clas, drop_mult=0.5)\n",
        "learn.load_encoder('ft_enc')\n",
        "learn.freeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0f4YIBcKe-1",
        "colab_type": "text"
      },
      "source": [
        "Note that we begin the training with all the outer layers as frozen. The fine-tuned language model is read in, and we use a dropout multiplier as shown above. \n",
        "\n",
        "For training the classifier, we apply \"gradual unfreezing\", to avoid \"catastrophic forgetting\" as explained in section $3.3$ of [Howard and Ruder's paper](https://arxiv.org/pdf/1801.06146.pdf). In addition to gradual unfreezing, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQD7U3whKe-2",
        "colab_type": "text"
      },
      "source": [
        "### Just as before, find optimum learning rate for the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gBIEG7FKe-2",
        "colab_type": "code",
        "outputId": "a74dd50a-5960-49b1-97f3-3d710e145ceb",
        "colab": {}
      },
      "source": [
        "learn.lr_find(start_lr=1e-8, end_lr=1e2)\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4VOX1wPHvyQIhAQJkYQtL2BcBgaggKrhVqlarVisudUGprdZqrbXWX8XWtnbRtrYWKaWItS51QesuWmVRQAggIYR9T1gSsu/r+f0xEx1CJpmQmcyd5HyeZx5m7nvv3JNLMmfe5b6vqCrGGGNMc8KCHYAxxpjQYAnDGGOMTyxhGGOM8YklDGOMMT6xhGGMMcYnljCMMcb4xBKGMcYYnwQsYYjIIhHJFpF0L+XXi0iaiGwWkVUiMsGj7F4R2SIi6SLyoohEBSpOY4wxvglkDWMxMLOJ8r3AdFUdBzwKLAAQkf7A3UCKqp4ChAPXBjBOY4wxPogI1Bur6goRGdxE+SqPl2uAJI/XEUAXEakGooFDvpwzPj5eBw/2ekpjjDENrF+//piqJviyb8ASRgvNBt4DUNUsEXkcOACUA0tVdakvbzJ48GBSU1MDF6UxxrQzIrLf132D3uktIufiShgPuF/3BC4HkoF+QIyI3NDE8XNEJFVEUnNyctoiZGOM6ZCCmjBEZDywELhcVXPdmy8A9qpqjqpWA0uAM729h6ouUNUUVU1JSPCpVmWMMeYkBC1hiMhAXMngRlXd4VF0AJgiItEiIsD5wNZgxGiMMeYrAevDEJEXgRlAvIhkAnOBSABVnQ88DMQB81x5gRp3TeFzEXkV2ADUABtxj6AyxhgTPNKe1sNISUlR6/Q2xhjfich6VU3xZd+gd3obY4wJDZYwjDHG+MQShjHGhLAPM44yf/nuNjmXJQxjjAlhH2w5wrOr9rXJuSxhGGNMCCsoq6JndKc2OZclDGOMCWF5pVX0jIlsk3NZwjDGmBCWX1ZtNQxjjDHNyy+roleMJQxjjDFNqKmto7C8mh5WwzDGGNOUwvJqVKFXtPVhGGOMaUJ+WTUAPa1JyhhjTFPyy6oArNPbGGNM0/JKXQnDOr2NMcY0qaC+hmEJwxhjTFPySt19GNbpbYwxpikFZVV0jgijS2R4m5wvYAlDRBaJSLaIpHspv15E0kRks4isEpEJHmU9RORVEdkmIltFZGqg4jTGmFCVV+q6ac+9amnABbKGsRiY2UT5XmC6qo4DHuX4ZVifBN5X1VHABGxNb2OMOUF+WVWb3bQHAVzTW1VXiMjgJspXebxcAyQBiEgscA5ws3u/KqAqUHEaY0yoyi+rplcbTTwIzunDmA28536eDOQAz4jIRhFZKCIxwQvNGGOcKb+07aY2BwckDBE5F1fCeMC9KQKYBDytqhOBUuCnTRw/R0RSRSQ1Jycn4PEaY4xT5LXhWhgQ5IQhIuOBhcDlqprr3pwJZKrq5+7Xr+JKII1S1QWqmqKqKQkJCYEN2BhjHKK2Tiksr26zezAgiAlDRAYCS4AbVXVH/XZVPQIcFJGR7k3nAxlBCNEYYxyrrScehAB2eovIi8AMIF5EMoG5QCSAqs4HHgbigHnuIWE1qpriPvwHwPMi0gnYA9wSqDiNMSYU1U8L0pY1jECOkprVTPltwG1eyr4AUhorM8YY4zEtSEfpwzDGGHNy2nriQbCEYYwxIal+avMebdiHYQnDGGNCUP3iSVbDMMYY06T80radeBAsYRhjTEjKc9/l3VYTD4IlDGOMCUn5ZW170x5YwjDGmJCUX1bVphMPgiUMY4wJSfmlbTu1OVjCMMaYkJRfVkUvSxjGGGOaUlunFLTxxINgCcMYY0JO/cSDPdvwpj2whGGMMSGn/i7vtrxpDyxhGGNMyMkvbfuJB8EShjHGhJw8SxjGGGN8UeCeR6qn3YdhjDGmKXntrQ9DRBaJSLaIpHspv15E0kRks4isEpEJDcrDRWSjiLwdqBiNMSYU5ZdW0amNJx6EwNYwFgMzmyjfC0xX1XHAo8CCBuU/BLYGJjRjjAld9TftteXEgxDAhKGqK4C8JspXqWq+++UaIKm+TESSgEuAhYGKzxhjQlVeadvftAfO6cOYDbzn8frPwE+AuuCEY4wxzpVfVtXmN+2BAxKGiJyLK2E84H59KZCtqut9PH6OiKSKSGpOTk4AIzXGGGfIL6vqeDUMERmPq9npclXNdW+eBlwmIvuAl4DzROTf3t5DVReoaoqqpiQkJAQ8ZmOMCbb80rafeBCCmDBEZCCwBLhRVXfUb1fVB1U1SVUHA9cCH6vqDUEK0xhjHCVYEw8CRATqjUXkRWAGEC8imcBcIBJAVecDDwNxwDx3T3+NqqYEKh5jjGkPioI08SAEMGGo6qxmym8Dbmtmn2XAMv9FZYwxoS1YN+2BAzq9jTHG+C5YEw+CJQxjjAkp+fXzSFnCMMYY05QvaxhtPPEgWMIwxpiQYn0YxhhjfJJfFpyJB8EShjHGhJT6m/baeuJBsIRhjDEhJVgTD4IlDGOMCSkFQZp4ECxhGGNMSMkL0sSDYAnDGGNChqqSW2I1DGOMMc3YnVNKYXk1o/t2D8r5LWEYY0yIWL7DtebPOcODs5SDJQxjjAkRK3bkMCQhhgG9ooNyfksYxhgTAiqqa1mzJzdotQuwhGGMMSFh7d48KmvqmD7SEoYxxpgmrNiRQ6eIMKYkxwUtBksYxhgTApbvyOH0wb3o0qnt55CqF7CEISKLRCRbRNK9lF8vImkisllEVonIBPf2ASLyiYhkiMgWEflhoGI0xphQcKignJ3ZJUwfEbzmKAhsDWMxMLOJ8r3AdFUdBzwKLHBvrwHuU9UxwBTgThEZE8A4jTHG0VbudA+nba8JQ1VXAHlNlK9S1Xz3yzVAknv7YVXd4H5eDGwF+gcqTmOMcbrlO3Lo0z2KEb27BjUOp/RhzAbea7hRRAYDE4HPvR0oInNEJFVEUnNycgIWoDHGBENNbR2f7jzGOSPigzKluaegJwwRORdXwnigwfauwGvAPapa5O14VV2gqimqmpKQENzqmjHG+NumzAKKKmqC3hwFEBHMk4vIeGAh8HVVzfXYHokrWTyvqkuCFZ8xxgTb8h3HCBM4a1h8sEMJXg1DRAYCS4AbVXWHx3YB/glsVdU/Bis+Y4xxghU7cpgwoAc9ooMzpbmnQA6rfRFYDYwUkUwRmS0id4jIHe5dHgbigHki8oWIpLq3TwNuBM5zb/9CRC4OVJzGGONU+aVVbMosCOp0IJ4C1iSlqrOaKb8NuK2R7Z8Cwe3ZMcYYB/h4WzaqBHU6EE9B7/Q2xhjTuCUbMxnYK5qJA3oEOxTAEoYxxjjSoYJyVu3O5cpJ/YM+nLaeJQxjjHGg1zdmoQpXTkwKdihfsoRhjDEOo6os2ZDJ6YN7MTAuOIslNcYShjHGOMymzEJ255Ry5SRnzYpkCcMYYxxmyYZMOkeEcfH4vsEO5TiWMIwxxkEqa2p5c9Mhvja2D92jIoMdznEsYRhjjIN8si2HgrJqxzVHgSUMY4xxlCUbMkno1pmzHTB3VEOWMIwxxiHySqv4ZHs23zy1HxHhzvt4dl5ExhjTQb216RDVtcpVk51z74UnSxjGGOMQa/fmMbBXNKP6dA92KI2yhGGMMQ6RV1pFYrfOwQ7DK0sYxhjjEPllVY5Y98IbSxjGGOMQ+WVV9Ipx1r0XnixhGGOMA6gq+WXV9OyINQwRWSQi2SKS7qX8ehFJE5HNIrJKRCZ4lM0Uke0isktEfhqoGI0xxinKqmqpqqmjZ0yIJwwRGSoind3PZ4jI3SLS3Ioei4GZTZTvBaar6jjgUWCB+/3Dgb8BXwfGALNEZIwvcRpjTKjKL6sCoGd06DdJvQbUisgwXB/sA4AXmjpAVVcAeU2Ur1LVfPfLNUD9wOPTgV2qukdVq4CXgMt9jNMYY0JSfmk1QLtokqpT1RrgCuCvqno/4M9pFGcD77mf9wcOepRlurc1SkTmiEiqiKTm5OT4MSRjjGk7X9YwQr1JCqgWkVnATcDb7m1+qTeJyLm4EsYDJ3O8qi5Q1RRVTUlIcMZC6cYY01JfNUmFfsK4BZgK/FpV94pIMvBca08uIuOBhcDlqprr3pyFq8mrXpJ7mzHGtFv5pc7vw4jwZSdVzQDuBhCRnkA3Vf1da04sIgOBJcCNqrrDo2gdMNydlLKAa4HrWnMuY4xxuvyyakQgtkuIJwwRWQZc5t5/PZAtIp+p6o+aOOZFYAYQLyKZwFzczViqOh94GIgD5okIQI27aalGRO4CPgDCgUWquuXkfjxjjAkN+WVVdI+KdOQstfV8ShhArKoWichtwL9Uda6IpDV1gKrOaqb8NuA2L2XvAu/6GJsxxoS8/LJqejm4wxt878OIEJG+wDV81eltjDHGT/JLq+jh4P4L8D1h/BJXE9FuVV0nIkOAnYELyxhjOpb8sip6OXiEFPje6f0K8IrH6z3AVYEKyhhjOpr80irHroNRz9epQZJE5HX33FDZIvKaiDhzSShjjAlBrj6M9tEk9QzwJtDP/XjLvc0YY0wrVVTXUl5d6+i1MMD3hJGgqs+oao37sRiw26qNMcYP6u/ybi+jpHJF5AYRCXc/bgBymz3KGGNMs/JC4C5v8D1h3IprSO0R4DDwLeDmAMVkjDEdSkGZa6badtEkpar7VfUyVU1Q1URV/SY2SsoYY/yivobRXpqkGuN1WhBjjDG+K3D3YbSXG/caI36LwhhjOrC8EFg8CVqXMNRvURhjTAeWX1ZFt84RRDp44kFo5k5vESmm8cQgQJeARGSMMR1MflmVo1faq9dkwlDVbm0ViDHGdFT5ZdWOH1ILrWuSMsYY4wf5paFRw7CEYYwxQZZfVuX4Dm8IYMIQkUXuiQrTvZSPEpHVIlIpIj9uUHaviGwRkXQReVFEogIVpzHGBFtBWXXHThjAYmBmE+V5uNYJf9xzo4j0d29PUdVTcC3Tem2AYjTGmKCqqqmjpLKmY/dhqOoKXEnBW3m2qq4DqhspjgC6iEgEEA0cCkyUxhgTXPU37VkfxklQ1SxctY4DuOatKlTVpd72F5E5IpIqIqk5OTltFaYxxvhFXn3C6OBNUidFRHoClwPJuNbeiHHPjtsoVV2gqimqmpKQYDOuG2NCS379Xd4OXzwJHJgwgAuAvaqao6rVwBLgzCDHZIwxAZFvNYxWOQBMEZFoERHgfGBrkGMyxpiACJXFk6CZO71bQ0ReBGYA8SKSCcwFIgFUdb6I9AFSge5AnYjcA4xR1c9F5FVgA1ADbAQWBCpOY4wJpvzS0JipFgKYMFR1VjPlR4AkL2VzcSUYY4xp1/LLqonpFE7niPBgh9IsJzZJGWNMh5FfWuX4lfbqWcIwxpggyi+rCon+C7CEYYwxQZVXVh0S/RdgCcMYY4KqIEQmHgRLGMYYE1R5pdYkZYwxphnVtXUUV9RYk5QxxpimFZS5pgWxGoYxxpgm1c9Ua8NqjTHGNCm/voZhCcMYY0xT8kJoWhCwhGGMMUFTEEITD4IlDGOMCZpQWjwJLGEYY0zQFJRVExUZRpdOzp94ECxhGGNM0OSVhs5d3mAJwxhjgiaUpgUBSxjGGBM0eaVVIbGWd72AJQwRWSQi2SKS7qV8lIisFpFKEflxg7IeIvKqiGwTka0iMjVQcRpjTLAUlFVbDcNtMTCzifI84G7g8UbKngTeV9VRwARsTW9jTDuUZ01SLqq6AldS8FaerarrgGrP7SISC5wD/NO9X5WqFgQqTmOMCYayqhoKy6uJ79o52KH4zIl9GMlADvCMiGwUkYUiEuNtZxGZIyKpIpKak5PTdlEGyebMQhau3ENdnQY7FGNMK2w9XIwqjOnXPdih+MyJCSMCmAQ8raoTgVLgp952VtUFqpqiqikJCQltFWNQvLzuIFfNX8Wv3tnK7z/YHuxwjDGtsOVQIQCn9A+dhBER7AAakQlkqurn7tev0kTC6Aiqaur4xVtbeP7zA5w1LJ6+sVHMX76boQkxXJ0yINjhGWNOwubMQuJiOtGne1SwQ/GZ4xKGqh4RkYMiMlJVtwPnAxnBjitYjhZV8P3nN7B+fz53TB/K/ReNpE6VQ4Xl/Oz1zQzsFc0ZQ+KCHaYxpoXSDxUxtn8sIhLsUHwWsIQhIi8CM4B4EckE5gKRAKo6X0T6AKlAd6BORO4BxqhqEfAD4HkR6QTsAW4JVJxO9k7aYX7+33Qqqmt56rqJXDq+HwDhCPOum8wVT3/GHf9ezxt3TmNQnNduHnMSVJVlO3J4bvV+iiuqqVOoU6VOYWy/7tx+9hCS4+2am5NTUV3LzqPFnDcqtJrRA5YwVHVWM+VHgCQvZV8AKYGIKxTkllTy8JtbeCftMOP6x/LHayYwvHe34/aJjY5k0U2n8c15n3Hr4nUs+f40YrsE9wagTQcLWLxqHz+6cAQDekUHNZbWWLX7GE8s3cH6/fn0i41iUFwMYWEQJoIqvLo+kxfXHuDiU/pyx/ShjEuKDXbIQVNSWcPOo8XklVZx9vAEOkU4sVvUeXYcLaamTjmlX2j97jiuSaqjez/9MA+9nk5RRTX3XzSS754zhIjwxv8IB8fHMP+GyVy/8HMee3crv71qfBtH+5X1+/O5edFaiitrWLnzGItuTmF8Uo+gxXMy1u/P54ml21m1O5c+3aP49RWncPXkASd8CGYXV/DMZ/v49+r9vLP5MNNHJPDIZWO91jhS9+VRXl3L1CFxXv8vT8bKnTkMS+xK39gufntPX6zYkcOzq/ax/WgxmfnlX26fOiSOp2+YFDKrxwXT5qz6Du/QShii2n6GZ6akpGhqamqwwzhp85bt4vfvb2dc/1gev3oCI/t0a/4g4DfvbmXBij28esdUUgb3CnCUJ1q7N49bnllLYvcoHr38FH66JI3ckiqeum4i54/u3ebxtNSWQ4U8sXQHH2/LJr5rJ74/YxjXnTGQqMimZxAtqqjm+TUHmLdsF1U1dfzowhHMPiv5y6SwJ6eE37y7lY+2ZgOQ0K0zV0zsz5WT+jOqT9MjY2rrlDDBa/v285/v56HX0+kUEcbNZw7me9OH0rMN1lT4ZFs2c55LJbFbFJMG9WRk766M6N2NYyVVPPLmFvr37MI/b0phSELXgMcSyh5cspl3Nx/mi4cvDHofhoisV1WfWnQsYTjEUx/v5PGlO7hsQj+euGYCkS34JlpWVcOFf1xB184RvH33WS06trVW787l1sXr6Ncjihdun0Lv7lFkF1cwe3EqWw4V8ovLT+HGKYPaLJ6W2JNTwhNLd/DO5sPEdonku9OHcPOZg4nu1LKK99GiCv7vjXQ+zDjKuP6x/PzSMSzdcoRnV++jc0Q4d503jEG9onltQxbLtmdTU6cMS+zKsISuJPXsQlLPLvSJjeJQQQVbDxex7UgxO44WMyShKwtunHxC897yHTncungd04bFk9C1M0s2ZtK1cwR3TB/KLdNaHr+njQfyeeC1NG6Zlsy3UwYQFvbVh9lnu45xy+J1jOjdledvm3JCE2jqvjzmPLee2jrl6esnceaw+JOOo7277KlP6RYVwfO3TQl2KJYwnKi8qpYDeWXEd+1EXIM7O//80Q7+/NFOrpzYnz9cPYHwsJZ/4/gw4yi3/yuVB2aO4nszhrb4+IrqWrKLKhHB/RBiOoV7bV6oqa3jnc2HeeC1NAb0jOaF26eQ0O2rn6usqoYfvLCR/23L5oqJ/Xlg5ij6xDpn+GB6ViGzFqyhTpXZZyUz++whreoDUlXe3XyEuW+mc6ykChG4ZvIA7rtoBIndvvq5c0sqeXPTIZbvyOFgXhmZ+eVU1tR9WR7ftROj+3ZnaEJXXtuQSeeIMBbedBqnDnA17209XMTV81czsFc0L98xla6dI9h+pJg/fLCNj7Zm06d7FI9cNoaLxvZp8TfXmto6vvHUZ+w4WkxtnXJGci8eu3IcQxK68vmeXG56Zi2D42J48fYpXmszB/PKuHXxOvYeK+XHF43k1mnJQe3XqK6t4/EPtnPdGQMdMzCkuraOsQ9/wC3TBvPgxaODHY4lDCfIOFTEC2v3s/NoCftySzlaVAm4PoxPHdCD80Ymct7oRD5IP8JfPt7FtyYn8burxp9Usqg351+prNiZw4f3Tve503l/bin/Wr2fl1MPUlxRc0L5hAE9uGhsb2aO7cOQhK5kFZTzn7UH+E/qQY4WVTK6b3eem316o9Mb1NTW8eT/dvL35XsIDxO+N2Moc84Z0mxTT6DtPVbK1fNX0TkinJfvmEr/Hv7rA8gvreLfa/Zz7qhEn9qnVZVjJVUcKaygT2zUcUl3V3YxtyxeR3ZRJX/+9qlMGtSTb/7tM1ThjTunnZCA1+3L4+dvpLPtSDHnj0rkkcvGtmjwwbOr9jH3zS387bpJlFRW86t3tlJZU8eNUwbx0toD9ImN4qU5U4+LsTFFFdX8+OVNLM04ypCEGOZ+YyzTRwRnNNCqXce4buHnnDk0judvOyPozT/gagK95C+f8pdZE7lsQr9gh2MJoy0czCujqraOwXExx33Ip2cV8uT/dvJhxlGiO4Uzum93BsfFkBwfzYBe0ew7VsbH246yKbPwy2O+nTKAx64cd1z1/2QcKijnwj8u5/TkXiy6+TRKKmt4L/0Ir2/IYnNWIYPiohmW2JXhiV3pE9uFdzcf5pPt2YSL8PVxfTl7uLsJQUFRcoor+TDjq1gH9OpCVn45CkwfkcB1pw/kvFGJzXbkHsgt47H3tvJe+hH6xUbxs0tGc8m4vkH54z1aVMFVT6+irKqWV+6YylCHt7XnllRy+79S2XCggH6xURSUV/PKHVMZ62V0TU1tHc98to8/fbSDOlV+eP4Ibjs7udlmymMllZz3+DLGJ/XgudmnIyJkF1Uw980tvJd+hEFx0fxnztQW1RI/3naUX76Vwb7cMi4c05s7pg+htg6KK6oprqihorqWEX26cUq/2IDVQh57byt/X74HgPk3TGbmKX0Ccp6WeHndQX7yWhof3zfdEX09ljAC7EhhBRf8cTkllTV0iQxnVN9ujOnbnaNFFXy0NZvuURHcelYyt0xL9trMkV1cwbJtOdSqntBW3BoLV+7hV+9s5ezh8azbl0dFdR2D4qKZNiyerPxydmWXkFXgGtkS37Uz150xkOvPGEjvJu42zSooZ+mWI6zceYwxfbvz7dMGnNSw2TV7cvnlWxlkHC7igtGJ/Oqb49q0maqwrJpr/r6azPwyXpwzJWRGcVVU13L/q2m8u/kw//jOZM4b1fxAgqyCcn7x5haWZhxldN/u/O6qcU3+vD95dRNLNmTx/j1nMyzx+MEWa/fmkRwf02zNojGVNbUs+nQff/14J2VVtY3u0zkijAlJPZg0qCfXpCT59UP04idXEt0pnOKKGsqqa/jw3ulBr+E+/N90lmzIIm3u1/z2d98aljAC7K4XNrA04yj/d8lo9h4rJeNQERmHiwgTYfZZydw8bTDdo4JzT0RNbR1XzV/NvmOlXDq+L1dOSmLSwB7HfZsvrazhYH4ZyfExdI5o2z+e2jrlmc/28vjS7USGhfGzS0Zz7WkDAl7bKKqo5tZn1pGWWcgzt5zGtBDrkFVVispriI1u2e/V++lHePi/6RwrqeS2s4dw7wUjTlg/esOBfK6ct4rvnjMkYG3q2UUVbDhQQNfOEXSLcj0iwsLYcqiQ1P35rN+fz5ZDhcR26cQbd55JUs/W38eTU1zJab/+iPsvGsmpA3pw/cLPuf+ikdx57jA//EQn78p5nxERHsbL33XGMj+WMALos13HuH7h5/zw/OHce+GIL7erKqo44htDda2rE7UtR0u11P7cUn762mZW78nlzKFxPH3D5IDdeLjlUCHff34Dmfnl/HXWRC4e1zcg53GqwvJqfvveNl5ce4CBvaK5YcpAJg7sybj+sUSGh3H53z4lp7iS/903g66dg3dr1q7sYq6Yt4p+sV145XtTW/2l6/WNmdz7n028dddZjEuK5bvPpbJy5zE+vm9G0AZg1NYpY+e+z3WnD+Lhb4wJSgwNtSRhOPcTxYEqa2r5+X/TGRQXfcJIJBFxRLIAV6JwcrIAGBQXwwu3n8FjV45j7d48fvLqJvz95UVVeWntAa6Yt4rK6jr+M2dKh0sWALFdInnsynG8NGcK0Z3C+c2727h6/mpOmfsBF/xxOelZRTx0yZigJguAYYndmH/DZHbnlHDn8xu+/OJzslbuOEavmE6MdU8f/tDFY6ipVX7//jZ/hHtSdueUUFFdF1Iz1Hpy9qeKwyxcuZc9OaU8ctnYoLeDtgciwqzTB/LTr4/igy1H+eene/323uVVtfz4lTR+umQzZyT34p27zwrKTY1OMmVIHO/fcw7rHrqAf3wnhTnnDKFP9yiumpTEN8Y7I5FOGxbPr684hZU7jzH3zS3HfYk4VFDOJ9uyySutavZ96uqUFTuPcdaw+C+/yA2Mi+a2s5NZsjGLT7Znc6ykkvKqWr9/UWlKeoje4V3PpgbxUWZ+GX/9eCcXje3NuSMTgx1OuzL7rGTW7s3jt+9tY+LAnkwe1LNV73e0qIJbF68j43ARPzx/OHefP7xVw5Xbm4RunblwTG8uHOPMu/C/fdpA9uWW8fSy3dTVKaVVtazfl8ehwgoAIsOF80YlctWkJM4dldhobXrbkWKOlVR+NfLP7c5zh/HahkxueWbdl9vCBIYmdOX528847p6ZQEjPKiIqMszxo/O8sYTho1++lYEgPPyNscEOpd0REf5w9QQu/etK7nphA+/cfTa9Yjq5ZozdnsO8Zbs4VFDBpRP6cvXkAQxL9P7HlnGoiNnPrqOovJqF30kJialJzInu/9pIDuSW8dK6g/TpHsXkwT25fVBPhid2Y9n2bN74IosPthwlLqYTD3x9FNc0WBdm5U7X6pvnNLj/I6ZzBEu+P401u3Mpq6qhtKqW4opq/rFyL794K4O/XTcpoD9X+qFCxvTtHrJfYCxh+OCLgwUszTjKT2aO9OtNXuYrsV0imXfdZK56ehX3/ucLrj1tAE99sosth4ro36MLI3p3ZeHKvfx9+R4mDezBVZOTOCM5jiHxMV82OXyyLZu7XthAt6hIXrnjzJBa+tIcLyxM+MuP1mwKAAARDElEQVSsicz9xhgSGwz5Pmt4PA98fRQrduQwb9lu/u/1dCYP6nnct/YVO3MY2btbo8PF+/fowlWTj58oOyoinCc+3MGVE48G7EtGXZ2ScaiIKyf1D8j7twVLGD5YuzcX4IRvMca/xiXF8vNvjOHnb6SzfEcOyfEx/P5b47liYn8iw8PILq7gjY1ZvJKayUOvpwMQ0ymcsf1iSerVhTc2ZjG6b3f+edNpjpqGxJyc8DA5IVnUiwwP4/zRvRmXFMsFTyznZ0s289KcKYgI5VW1rNubz3em+j6H2XenD+XttMP83xvpnDEkLiADAPblllJSWRNyU5p7soThg7TMQvr36NLo9BfGv244YyDVNXUkdOvMxeP6Hld1T+wWxZxzhnL72UPYmV3CpoMFpGcVkpZVyHubj3DR2D48fvUEYoI82se0ncRuUTx48WgeXLKZV1Izuea0AazZm0tVbd0JzVFN6RQRxm+uHMe35q/i8Q+288hl/m96Tj9UBMDYEB0hBYFdcW8RcCmQraqnNFI+CngGmAQ8pKqPNygPx7UiX5aqXhqoOH2RllnI+A68SE5bEhFuPSu52X1G9O7GiN7dvlzTXFUdMU+QaXvfThnA6xuy+PW7Wzl3VCIrdxyjc0QYpye3bFTc5EE9uXHKIJ5dvY/LTu3HpIGtG3zRUMahIiLDheGJvi1b4ESBHFa7GJjZRHkecDfwuJfyHwJb/RxTixWUVXEgr6xDr6oWCixZdFxhYcJvrjyF8qpaHn07g5U7czg9uddJDX2//6KR9O4WxYOvbaaqpnX3gTSUcbiI4YndQnpVwoBFrqorcCUFb+XZqroOqG5YJiJJwCXAwkDF56s098R7E0Jk3iFjOqJhid343oyhvLnpEDuzSzhn+MnNjtstKpJHv3kK248W8+jbGX69RyPjUFHID8Rwaqr7M/AToNkULyJzRCRVRFJzcnL8HkhaZgEQujfaGNNRfP/coQxJcK15cfaIk58r7MIxvbn97GSeW7OfP3200y+xZRdXcKykkjF9QzthOK53UETq+z3Wi8iM5vZX1QXAAnDNJeXveNIyC0mOjwnYPEfGGP/oHBHOX2dN5L3NRxjZu3X9BD+7eDSF5dX85X876dElstl+teZsPVwMEPI1DMclDGAacJmIXAxEAd1F5N+qekMwgknLLOSMIR17SgljQsXYfrFe1wppCRHhN1eMo6i8hl++nUFsl8gT7t1oiQz3CKnRzazl7nSOa5JS1QdVNUlVBwPXAh8HK1lkF1VwpKiCcdYcZUyHExEexpOzTmXasDh+8loaTy/bzSfbstlyqJBjJZXU1fneoJFx2HUDakunp3eaQA6rfRGYAcSLSCYwF4gEUNX5ItIH17DZ7kCdiNwDjFHVokDF1FJfdngPsA5vYzqizhHhLLgxhZsWreV3DWa5jYvpxHOzz/CpmSnjUGHIN0dBABOGqs5qpvwI0GQdT1WXAcv8F1XLpGUVEiZ8OT2yMabjiekcwcvfncrhogqOFlW4Wh4KK5i/fA+3PbuO/951VpOrEZZV1bDnWCmXjg/++t2t5cQ+DMdIyyxgeGI3ojvZZTKmIwsLE/r36HLcXHIpg3vxrfmr+O5zqbxw+xSv931sP1KMauh3eIMD+zCcQlXtDm9jjFen9I/lT9ecyoYDBTy4ZLPXezYyDrta2UN9SC1YwvAqq6CcvNIqxlv/hTHGi6+P68t9F47g9Y1ZzFu2u9F9Mg4V0S0qgqSeoT/TtSUML+o7vMfbCCljTBPuOm8Yl03oxx8+2M7yHSfePJxxuIgxfbu3i+lrLGF4kZZZSGS4MKpv6E4UZowJPBHh998az8Be0Tz50Y7jymrrlG2Hi9tF/wVYwvAqLbOAUX260znC1u42xjQtKjKcW6cNZsOBAjYcyP9y+/7cUsqra9tF/wVYwmhUXZ2y2Tq8jTEtcHXKALpFRfDPT/d+ua2+w3u0JYz2a19uKcWVNTZDrTHGZzGdI7jujIG8t/kwmfllgKvDOyJMGN7b+zr0ocQSRiPqO7xtDQxjTEvcNHUwIsKzq/YBrhrGsMSu7aZp2xJGI744WEBUZBjDE9vHtwJjTNvo16MLl4zry0trD1JSWdMu1sDwZAmjEZ/uOsZpg3sREW6XxxjTMrPPSqa4soZ5n+wiuzj018DwZJ+IDRzMK2NXdgkzRiYGOxRjTAiaMKAHpw3uyYIVe4D2MSVIPUsYDSzbng3AuSNPbolHY4yZfVYyNe7pz9tTDcNm1Wtg2fYcBsVFkxwfE+xQjDEh6sIxfRjQqwt1ddAjulOww/EbSxgeKqpr+Wz3Mb6dMqBd3MZvjAmO8DDhqVmTKKmsCXYofmUJw8Pne/OoqK5jxijrvzDGtE57XHgtYH0YIrJIRLJFJN1L+SgRWS0ilSLyY4/tA0TkExHJEJEtIvLDQMXY0CfbsukcEcbUIXFtdUpjjAkZgez0XgzMbKI8D7gbeLzB9hrgPlUdA0wB7hSRMQGJsIFl27M5c2ic14VQjDGmIwtYwlDVFbiSgrfybFVdB1Q32H5YVTe4nxcDW4H+gYqz3t5jpezLLeNca44yxphGOXpYrYgMBiYCnzexzxwRSRWR1JycE+ei91X9cNoZIyxhGGNMYxybMESkK/AacI+qFnnbT1UXqGqKqqYkJJz8vROfbM9hSEIMA+OiT/o9jDGmPXNkwhCRSFzJ4nlVXRLo85VX1bJmTy7n2t3dxhjjleMShrhugPgnsFVV/9gW51y95xhVNXWWMIwxpgkBuw9DRF4EZgDxIpIJzAUiAVR1voj0AVKB7kCdiNwDjAHGAzcCm0XkC/fb/UxV3w1UrJ9syyG6UzinJfcM1CmMMSbkBSxhqOqsZsqPAEmNFH0KtNlt1qrKJ9uzOXNofLuZs94YYwKhw9/pXVFdx5lD45g2LD7YoRhjjKN1+ITRpVM4v//WhGCHYYwxjue4Tm9jjDHOZAnDGGOMTyxhGGOM8YklDGOMMT6xhGGMMcYnljCMMcb4xBKGMcYYn1jCMMYY4xNR1WDH4DcikgPsB2KBQvfm5p7X/xsPHGvhKT3fz9fyhtuaet0wxmDH6i2+5uL2d7zeyny9tk78PWi4za5t87F6Kz+Za9vYto5ybQepqm9rQ6hqu3sAC3x97vFvamvO42t5w21NvW4kxqDG6pRr663M12vrxN8Du7bBvbZetnXoa9vYo702Sb3Vguee21pzHl/LG25r6nXDGIMda8Ntwbq23sp8vbZO/D1ouM2urW/H+uvaeitvqfZ0bU/QrpqkWkNEUlU1Jdhx+CKUYoXQijeUYoXQijeUYoXQiretYm2vNYyTsSDYAbRAKMUKoRVvKMUKoRVvKMUKoRVvm8RqNQxjjDE+sRqGMcYYn7TLhCEii0QkW0TST+LYySKyWUR2ichf3GuMIyL/EZEv3I99HsvHOi5Wd9kPRGSbiGwRkd/7I9ZAxSsij4hIlsf1vdipsXqU3yciKiJ+W3krQNf2URFJc1/XpSLSz8Gx/sH9O5smIq+LSA8Hx3q1+2+rTkT80nfQmji9vN9NIrLT/bjJY3uTv9tNaumwqlB4AOcAk4D0kzh2LTAF1zKx7wFfb2SfJ4CHnRorcC7wEdDZ/TrRydcWeAT4caj8HgADgA9w3fMT7+R4ge4e+9wNzHdwrF8DItzPfwf8zsGxjgZGAsuAlGDG6Y5hcINtvYA97n97up/3bO53u7lHu6xhqOoKIM9zm4gMFZH3RWS9iKwUkVENjxORvrj+wNao68r+C/hmg30EuAZ40cGxfg/4rapWus+R7Y9YAxhvQAQw1j8BPwH82gEYiHhVtchj1xh/xRygWJeqao171zVAkoNj3aqq2/0RX2vj9OIi4ENVzVPVfOBDYGZr/w7bZcLwYgHwA1WdDPwYmNfIPv2BTI/Xme5tns4GjqrqzoBE6dLaWEcAZ4vI5yKyXEROC2Cs4J9re5e7KWKRiPQMXKiti1VELgeyVHVTAGP01OprKyK/FpGDwPXAw06O1cOtuL79Boo/Yw0kX+JsTH/goMfr+thb9TN1iDW9RaQrcCbwikdzXeeTfLtZ+Kl20Rg/xRqBqyo6BTgNeFlEhri/UfiVn+J9GngU17ffR3E1+d3qrxjrtTZWEYkGfoar6STg/PV7q6oPAQ+JyIPAXcBcvwXp5s+/MRF5CKgBnvdPdCe8vz8/DwKmqThF5Bbgh+5tw4B3RaQK2KuqVwQqpg6RMHDVpApU9VTPjSISDqx3v3wT1weXZzU4Ccjy2D8CuBKY7PBYM4El7gSxVkTqcM01k+PEeFX1qMdx/wDeDkCc/oh1KJAMbHL/AScBG0TkdFU94sB4G3oeeJcAJAz89zd2M3ApcH4gvuD4M9Y20GicAKr6DPAMgIgsA25W1X0eu2QBMzxeJ+Hq68iiNT+TPzprnPgABuPReQSsAq52PxdggpfjGnYIXexRNhNY7vRYgTuAX7qfj8BVNRUHx9vXY597gZecGmuDffbhx07vAF3b4R77/AB41cGxzgQygAR/XtNA/h7gx07vk40T753ee3F1ePd0P+/l6++21/j8/R/jhAeuJqPDQDWub9uzcX0zfB/Y5P6lbHSUE5ACpAO7gafw+KAFFgN3OD1WoBPwb3fZBuA8h8f7HLAZSMP1za6vU2NtsM8+/DtKKhDX9jX39jRccwf1d3Csu3B9ufnC/fDXiK5AxHqF+70qgaPAB8GKk0YShnv7re5rugu4pSW/294edqe3McYYn3SkUVLGGGNawRKGMcYYn1jCMMYY4xNLGMYYY3xiCcMYY4xPLGGYdk1EStr4fAtFZIyf3qtWXLPMpovIW9LM7K0i0kNEvu+PcxvTGBtWa9o1ESlR1a5+fL8I/WqCvIDyjF1EngV2qOqvm9h/MPC2qp7SFvGZjsdqGKbDEZEEEXlNRNa5H9Pc208XkdUislFEVonISPf2m0XkTRH5GPifiMwQkWUi8qq41m943j2LMe7tKe7nJe6J/zaJyBoR6e3ePtT9erOI/MrHWtBqvpoAsauI/E9ENrjf43L3Pr8FhrprJX9w73u/+2dME5Ff+PEymg7IEobpiJ4E/qSqpwFXAQvd27cBZ6vqRFyzuv7G45hJwLdUdbr79UTgHmAMMASY1sh5YoA1qjoBWAHc7nH+J1V1HMfPHNoo9xxH5+O6Cx6gArhCVSfhWvvkCXfC+imwW1VPVdX7ReRrwHDgdOBUYLKInNPc+YzxpqNMPmiMpwuAMR4zgHZ3zwwaCzwrIsNxzZwb6XHMh6rquVbBWlXNBBDX6ouDgU8bnKeKryZSXA9c6H4+la/WIHgBeNxLnF3c790f2IprTQNwzQH0G/eHf527vHcjx3/N/djoft0VVwJZ4eV8xjTJEobpiMKAKapa4blRRJ4CPlHVK9z9Acs8iksbvEelx/NaGv9bqtavOgm97dOUclU91T2t+gfAncBfcK1rkQBMVtVqEdkHRDVyvACPqerfW3heYxplTVKmI1qKa+ZWAESkfvroWL6a6vnmAJ5/Da6mMIBrm9tZVctwLa96n3uK/Vgg250szgUGuXctBrp5HPoBcKu79oSI9BeRRD/9DKYDsoRh2rtoEcn0ePwI14dvirsjOAPXdPAAvwceE5GNBLb2fQ/wIxFJw7X4TWFzB6jqRlwzzs7Cta5FiohsBr6Dq+8FVc0FPnMPw/2Dqi7F1eS12r3vqxyfUIxpERtWa0wbczcxlauqisi1wCxVvby544wJNuvDMKbtTQaeco9sKiAAy9EaEwhWwzDGGOMT68MwxhjjE0sYxhhjfGIJwxhjjE8sYRhjjPGJJQxjjDE+sYRhjDHGJ/8PX2+sjlihUDYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmvs4caqKe-4",
        "colab_type": "text"
      },
      "source": [
        "### Carefully train the classifier\n",
        "During classification, we gradually unfreeze layers as shown below. This helps us obtain a better classification accuracy than if we were to aggressively train all the layers at once. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr3RxWeNKe-4",
        "colab_type": "code",
        "outputId": "44e76721-09fd-4e9e-afd0-5bbd53bace03",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(cyc_len=1, max_lr=1e-3, moms=(0.8, 0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:00 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>1.124041</th>\n",
              "    <th>1.034543</th>\n",
              "    <th>0.511278</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqm7zLZAKe-5",
        "colab_type": "code",
        "outputId": "e270e1c5-78a5-48f0-ef2a-4d586b969e73",
        "colab": {}
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-4,1e-2), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:00 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>1.031176</th>\n",
              "    <th>1.056985</th>\n",
              "    <th>0.488722</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYm3G5lXKe-7",
        "colab_type": "code",
        "outputId": "bef40998-91ad-43ce-ea80-01fab53b193c",
        "colab": {}
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(1e-5,5e-3), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:00 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>0.951832</th>\n",
              "    <th>1.075786</th>\n",
              "    <th>0.473684</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIZjr7n0Ke-8",
        "colab_type": "code",
        "outputId": "b864d7ea-00d6-42a0-a94e-adc7230a10a8",
        "colab": {}
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(4, slice(1e-5,1e-3), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:04 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>0.888372</th>\n",
              "    <th>1.051242</th>\n",
              "    <th>0.413534</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>2</th>\n",
              "    <th>0.859089</th>\n",
              "    <th>1.085068</th>\n",
              "    <th>0.451128</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>3</th>\n",
              "    <th>0.853765</th>\n",
              "    <th>1.041656</th>\n",
              "    <th>0.436090</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>4</th>\n",
              "    <th>0.808356</th>\n",
              "    <th>1.068729</th>\n",
              "    <th>0.406015</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F_rqnpgKe-9",
        "colab_type": "text"
      },
      "source": [
        "### Cross-tabulate the predictions for each class\n",
        "The below table shows the predicted classes for the topic in concern. The actual evaluation of the F1 score of the classification result is done at a later step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfOiO4CTKe--",
        "colab_type": "code",
        "outputId": "1d8f7bed-e72c-4547-d2b5-f8eebf1df932",
        "colab": {}
      },
      "source": [
        "# get predictions\n",
        "preds, targets = learn.get_preds()\n",
        "predictions = np.argmax(preds, axis=1)\n",
        "pd.crosstab(predictions, targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0   0   1   2\n",
              "row_0            \n",
              "0      30  45  23\n",
              "1       6  18   1\n",
              "2       0   4   6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q7QI6RqKe_A",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the predicted results from the classifier (SemEval tweets only)\n",
        "We read in the test dataset and store in a Pandas DataFrame as shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoA15dxjKe_B",
        "colab_type": "code",
        "outputId": "f5f62901-e91a-4a1b-baac-d950ae753a30",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv(path/testfile, delimiter='\\t', header=0, encoding = \"latin-1\")\n",
        "test = test.drop(['ID'], axis=1)\n",
        "# test['Tweet'] = test['Tweet'].apply(clean_ascii)\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Atheism</td>\n",
              "      <td>He who exalts himself shall      be humbled; a...</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Atheism</td>\n",
              "      <td>RT @prayerbullets: I remove Nehushtan -previou...</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Atheism</td>\n",
              "      <td>@Brainman365 @heidtjj @BenjaminLives I have so...</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Atheism</td>\n",
              "      <td>#God is utterly powerless without Human interv...</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Atheism</td>\n",
              "      <td>@David_Cameron   Miracles of #Multiculturalism...</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Target                                              Tweet   Stance\n",
              "0  Atheism  He who exalts himself shall      be humbled; a...  UNKNOWN\n",
              "1  Atheism  RT @prayerbullets: I remove Nehushtan -previou...  UNKNOWN\n",
              "2  Atheism  @Brainman365 @heidtjj @BenjaminLives I have so...  UNKNOWN\n",
              "3  Atheism  #God is utterly powerless without Human interv...  UNKNOWN\n",
              "4  Atheism  @David_Cameron   Miracles of #Multiculturalism...  UNKNOWN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Njfxz8Ke_D",
        "colab_type": "text"
      },
      "source": [
        "Since we do not yet know the stance for the test set, it is marked as \"UNKNOWN\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A97y7-lKe_F",
        "colab_type": "text"
      },
      "source": [
        "### Filter the test data set based on topic we are classifying for"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LNV863wKe_F",
        "colab_type": "code",
        "outputId": "ca9e2baa-f90d-4d8e-bdb7-1d2b5a6b0909",
        "colab": {}
      },
      "source": [
        "test_pred = test[['Target', 'Tweet']]\n",
        "test_pred = test_pred.loc[test_pred['Target'] == topic]\n",
        "test_pred.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>669</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>@Maisie_Williams is our hero with her #LikeAGi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>Rather be an \"ugly\" feminist then be these sad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>671</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>iamNovaah: RT ChrzOC: Bitches be running wild....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>@angerelle you disagree that people should str...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>#Rapeculture is basically a FABLE. It has almo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Target                                              Tweet\n",
              "669  Feminist Movement  @Maisie_Williams is our hero with her #LikeAGi...\n",
              "670  Feminist Movement  Rather be an \"ugly\" feminist then be these sad...\n",
              "671  Feminist Movement  iamNovaah: RT ChrzOC: Bitches be running wild....\n",
              "672  Feminist Movement  @angerelle you disagree that people should str...\n",
              "673  Feminist Movement  #Rapeculture is basically a FABLE. It has almo..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4202jiWXKe_H",
        "colab_type": "text"
      },
      "source": [
        "### Apply the learner's ```predict``` method to produce our prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTJ1xtYxKe_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred['Stance'] = test_pred['Tweet'].apply(lambda row: str(learn.predict(row)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vAf5MVhKe_I",
        "colab_type": "code",
        "outputId": "f07d6249-592d-4ca7-e8ef-7c62620f329a",
        "colab": {}
      },
      "source": [
        "test_pred.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>669</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>@Maisie_Williams is our hero with her #LikeAGi...</td>\n",
              "      <td>FAVOR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>Rather be an \"ugly\" feminist then be these sad...</td>\n",
              "      <td>FAVOR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>671</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>iamNovaah: RT ChrzOC: Bitches be running wild....</td>\n",
              "      <td>AGAINST</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>@angerelle you disagree that people should str...</td>\n",
              "      <td>AGAINST</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>Feminist Movement</td>\n",
              "      <td>#Rapeculture is basically a FABLE. It has almo...</td>\n",
              "      <td>AGAINST</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Target                                              Tweet  \\\n",
              "669  Feminist Movement  @Maisie_Williams is our hero with her #LikeAGi...   \n",
              "670  Feminist Movement  Rather be an \"ugly\" feminist then be these sad...   \n",
              "671  Feminist Movement  iamNovaah: RT ChrzOC: Bitches be running wild....   \n",
              "672  Feminist Movement  @angerelle you disagree that people should str...   \n",
              "673  Feminist Movement  #Rapeculture is basically a FABLE. It has almo...   \n",
              "\n",
              "      Stance  \n",
              "669    FAVOR  \n",
              "670    FAVOR  \n",
              "671  AGAINST  \n",
              "672  AGAINST  \n",
              "673  AGAINST  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7aa_qQjKe_K",
        "colab_type": "text"
      },
      "source": [
        "### Output the predicted dataset to a text file for comparison with the gold reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe9RB_-jKe_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred.to_csv(path/'eval'/'predicted.txt', sep='\\t', index=True,\n",
        "                 header=['Target', 'Tweet', 'Stance'], index_label='ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY88Iw1PKe_N",
        "colab_type": "text"
      },
      "source": [
        "Once we have the predicted data, we use the *perl* script provided by the SemEval organizers to generate the F1 score. \n",
        "\n",
        "The evaluation script is in ```data/eval/``` and has the following usage:\n",
        "\n",
        "    perl eval.py gold_feminism.txt predicted.txt\n",
        "    \n",
        "    ============\n",
        "    Results\t\t\t\t \n",
        "    ============\n",
        "    FAVOR     precision: 0.3265 recall: 0.5517 f-score: 0.4103\n",
        "    AGAINST   precision: 0.7259 recall: 0.5355 f-score: 0.6164\n",
        "    ------------\n",
        "    Macro F: 0.5133"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uib8vZ3DKe_O",
        "colab_type": "text"
      },
      "source": [
        "Using a relatively small input vocabulary from the Twitter training data, we do not do that well in the F-score for the \"Feminist Movement\" topic.  On trying out a similar approach for Tweets from the other topics (Atheism, Hillary Clinton, etc.), we still do not see an F-score of above 0.60 - this is well below the winning score by *MITRE* of 0.67.\n",
        "\n",
        "To improve the score, some more fine-tuning is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDEiRc06Ke_O",
        "colab_type": "text"
      },
      "source": [
        "## 5.3 Train the Classifier: Augmented Vocabulary\n",
        "Since the initial approach (despite multiple attempts at fine-tuning both the language model and the classifier) did not an F-score as high as *MITRE's* (which was ~0.67), it can be reasoned that this could be because the pre-trained language model did not generalize to Tweets specifically (since the language syntax of tweets are quite different than the Wikipedia text that the LSTMs were trained on). \n",
        "\n",
        "The provided training data of 2,800 Tweets can be augmented with the Kaggle [Twitter Sentiment140 dataset](https://www.kaggle.com/kazanova/sentiment140) of 1.6 million tweets. By fine-tuning the language model on this larger Twitter dataset, we might better learn the structure of Twitter conversations. \n",
        "\n",
        "**NOTE**: Since 1.6 million tweets is quite a large dataset and can add a significant number of unique, one-time-only words, in this notebook, we only sampled a subset of 200,000 from the original Sentiment140 dataset for the language model fine-tuning to save on language model fine-tuning time. It is possible that fine-tuning the language model with a larger subset of the augmentation vocabulary could further improve the language model fine-tuned results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7KJDJNkaKe_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Kaggle Twitter Dataset\n",
        "kaggle = pd.read_csv(path/\"training.1600000.processed.noemoticon.csv\",header=None,encoding = \"latin-1\")\n",
        "kaggle.columns = ['col1', 'col2',\"col3\",\"Stance\",\"col5\",\"Tweet\"]\n",
        "kaggle = kaggle[['Stance','Tweet']]\n",
        "# Clean Data and Remove URL's from the tweets\n",
        "kaggle['Tweet'] = kaggle['Tweet'].apply(clean_ascii)\n",
        "kaggle['Tweet'] = kaggle['Tweet'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
        "kaggle_sub = kaggle.sample(n=200000,random_state=77)\n",
        "kaggle_sub = kaggle_sub.append(train, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9U_n7X5vKe_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Lines are commented out as this takes a long time to run\n",
        "# Write train to csv\n",
        "kaggle_sub.to_csv(path/'kaggle_train_sub_v2.csv', index=False, header=False)\n",
        "twitter_lm = TextLMDataBunch.from_csv(path, 'kaggle_train_sub_v2.csv', min_freq=1)\n",
        "kaggle_lm.save()\n",
        "twitter_lm.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "m9Rb-_BHKe_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_twitter = language_model_learner(twitter_lm, pretrained_model=URLs.WT103_1, drop_mult=0.1)\n",
        "learn_twitter.lr_find(start_lr=1e-8, end_lr=1e2)\n",
        "learn_twitter.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U3LNMOmKe_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_twitter = language_model_learner(twitter_lm, pretrained_model=URLs.WT103_1,drop_mult=0.5)\n",
        "learn_twitter.fit_one_cycle(cyc_len=1, max_lr=1e-3, moms=(0.8, 0.7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mT_40PoVKe_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_twitter.unfreeze()\n",
        "learn_twitter.fit_one_cycle(cyc_len=10, max_lr=1e-3, moms=(0.8, 0.7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RVp2xEOKe_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the fine-tuned encoder (Uncomment the line given below)\n",
        "learn_twitter.save_encoder('ft_enc_kaggle_v1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95vX8e4jKe_Y",
        "colab_type": "text"
      },
      "source": [
        "### Load Vocabulary\n",
        "This assumes that we have a fine-tuned language model's numericalized vocabulary ```vocab.pkl``` in the same directory as the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiMt50UIKe_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the Vocab to a Pickle file for later use\n",
        "# pickle.dump(twitter_lm.train_ds.vocab, open(f'vocab.pkl','wb'))\n",
        "vocab_load = pickle.load(open('vocab.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2H9EoALKe_a",
        "colab_type": "text"
      },
      "source": [
        "Various values of momentum, dropout and learning rates were tried to find the best values of these parameters. The below parameters are applied in a for loop to train on each topic one by one, and then appending the predictions to a common file for comparison with the gold reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gwsdeLW7Ke_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for topic in train_orig.Target.unique():\n",
        "    print(\"Topic: \",topic)\n",
        "    #learn_twitter=learn_twitter_org\n",
        "        # Get only those tweets that pertain to a single topic in the training data\n",
        "    train_topic = train.loc[train_orig['Target'] == topic]\n",
        "    # Write train to csv\n",
        "    train_topic.to_csv(path/'train_topic.csv', index=False, header=False)\n",
        "    # Classifier model data\n",
        "    data_clas = TextClasDataBunch.from_csv(path, 'train_topic.csv', vocab=vocab_load, min_freq=7, bs=64)\n",
        "    data_clas.save()\n",
        "    learn = text_classifier_learner(data_clas, drop_mult=0.5)\n",
        "    learn.load_encoder('ft_enc_kaggle_v1')\n",
        "    learn.freeze()\n",
        "\n",
        "    #Find Optimum Learning Rate\n",
        "    learn.lr_find(start_lr=1e-6, end_lr=1e2)\n",
        "    learn.recorder.plot()\n",
        "\n",
        "    #Train the Classifier by Gradually Unfreezing the Layers\n",
        "    learn.fit_one_cycle(cyc_len=1, max_lr=1e-3, moms=(0.8, 0.7))\n",
        "    #Unfreeze Layer\n",
        "    learn.freeze_to(-2)\n",
        "    learn.fit_one_cycle(3, slice(1e-4,1e-2), moms=(0.8,0.7))\n",
        "    #Unfreeze Layer\n",
        "    learn.freeze_to(-3)\n",
        "    learn.fit_one_cycle(3, slice(1e-5,5e-3), moms=(0.8,0.7))\n",
        "    #Unfreeze All Layers\n",
        "    learn.unfreeze()\n",
        "    learn.fit_one_cycle(3, slice(1e-5,1e-3), moms=(0.8,0.7))\n",
        "    # get predictions\n",
        "    preds, targets = learn.get_preds()\n",
        "    predictions = np.argmax(preds, axis=1)\n",
        "    pd.crosstab(predictions, targets)\n",
        "\n",
        "    test = pd.read_csv(path/testfile, delimiter='\\t', header=0, encoding = \"latin-1\")\n",
        "    # test = test.drop(['ID'], axis=1)\n",
        "    # test['Tweet'] = test['Tweet'].apply(clean_ascii)\n",
        "    test.head()\n",
        "\n",
        "    test_pred = test[['Target', 'Tweet']]\n",
        "    test_pred = test_pred.loc[test_pred['Target'] == topic]\n",
        "    test_pred.tail()\n",
        "\n",
        "    test_pred['Stance'] = test_pred['Tweet'].apply(lambda row: str(learn.predict(row)[0]))\n",
        "    file_to_save=\"predicted_\"+str(topic.replace(\" \", \"_\"))+\".txt\"\n",
        "    test_pred.to_csv(path/'eval'/file_to_save, sep='\\t', index=True,\n",
        "                     header=['Target', 'Tweet', 'Stance'], index_label='ID')\n",
        "    if(topic==\"Atheism\"):\n",
        "        preds_document=test_pred\n",
        "    else:\n",
        "        preds_document=preds_document.append(test_pred, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD99v4oaKe_d",
        "colab_type": "text"
      },
      "source": [
        "### Output the predicted dataset to a text file for comparison with the gold reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rVtJ9DHKe_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_document.to_csv(path/'eval'/'predicted.txt', sep='\\t', index=True,\n",
        "                      header=['Target', 'Tweet', 'Stance'], index_label='ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0KhzJ1KKe_g",
        "colab_type": "text"
      },
      "source": [
        "The evaluation script is in ```data/eval/``` is run with the following command:\n",
        "\n",
        "    perl eval.py gold.txt predicted.txt\n",
        "\n",
        "    ============\n",
        "    Results                          \n",
        "    ============\n",
        "    FAVOR     precision: 0.5842 recall: 0.5362 f-score: 0.5592\n",
        "    AGAINST   precision: 0.7404 recall: 0.7301 f-score: 0.7352\n",
        "    ------------\n",
        "    Macro F: 0.6472"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbGM1f4dKe_g",
        "colab_type": "text"
      },
      "source": [
        "Based on the above result, we can see that augmenting the training data with additional domain-specific data (i.e., Tweets) helps us obtain better F-scores for the stance classification task.\n",
        "\n",
        "This result is much closer to the winning result by *MITRE* (of 0.67). The improvement can be largely attributed to the language model fine-tuning step, where we augmented the language model parameters with the tokenized form of a much larger Tweet vocabulary (of 200,000 Tweets downloaded from Kaggle). It can be reasoned that using an even larger Tweet vocabulary (say a million words) could result in a further improvement. However, this would also significantly increase the training time during fine-tuning. In addition to vocabulary augmentation, more extensive hyperparameter tuning and careful classifier fine-tuning could result in further gains. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZihWO00NKe_h",
        "colab_type": "text"
      },
      "source": [
        "# 6. Conclusions\n",
        "\n",
        "This notebook showed a training and classification pipeline for a PyTorch-based framework (*ULMFit*) for evaluating stance of Tweets towards a particular topic. It appears that with some basic data cleaning and augmentation with additional Twitter language data, the transfer learning approach using *ULMFit* can provide good classification accuracy F-scores (compared with the winning results from the SemEval 2016 winning team *MITRE*). \n",
        "\n",
        "The main benefit of using *ULMFit* is that it can perform classifier re-training with a *very limited amount of data* (fewer than 500 Tweets per topic in this dataset). However, since Twitter data is of a very different distribution from the pre-trained language model, the language model fine-tuning step is rather expensive (and can take significant time even on GPU). Once the language model is fine-tuned adequately, the classifier can perform per-topic classification relatively easily and with a reasonable level of accuracy comparable with the best score by *MITRE*. \n",
        "\n",
        "We can likely further improve the classification accuracy across all topics with the below steps:\n",
        "\n",
        "1.  Augment the language model vocabulary with a larger subset of Twitter Sentiment140 dataset to improve the model's domain understanding of Tweet syntax\n",
        "\n",
        "2.  Perform additional fine-tuning on the training hyperparameters in both the language model fune-tuning and the classifier fine-tuning stages. \n",
        "\n",
        "3.  Plug in a pre-trained language model that was trained on a large Twitter dataset (if such a model becomes available in the future!)\n",
        "\n",
        "Tweets are sufficiently different from typical language data used to generate pre-trained language models, and  hence are an interesting usecase for analyzing the effectiveness of transfer learning techniques. It will be interesting to see how transfer learning techniques for NLP tasks evolve with time."
      ]
    }
  ]
}